{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 1067156,
          "sourceType": "datasetVersion",
          "datasetId": 592212
        },
        {
          "sourceId": 1644506,
          "sourceType": "datasetVersion",
          "datasetId": 972382
        },
        {
          "sourceId": 1926230,
          "sourceType": "datasetVersion",
          "datasetId": 1148896
        },
        {
          "sourceId": 4165279,
          "sourceType": "datasetVersion",
          "datasetId": 2326821
        }
      ],
      "dockerImageVersionId": 30528,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salmabenhassin/CognoRise-Infotech-Projects/blob/main/Copie_de_Translate_EN_to_FR_(Tensorflow_%26_Transformers).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'language-translation-englishfrench:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F592212%2F1067156%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240911%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240911T112600Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6262d4056c2f4378db84f52e3b21e958bee3e0494ca73b72f9e2dd942f604a6d01ecc5c5c962ebe398247cdb8ebc68626b203584f66b12f4883e9c321e6b943770f6bfeac705dda3f53e37cbee6aecc8c6e684830e0f629f39f058b56142667956c37daf7f98270f8795b4d27634ae9d70fa6fe273c3910ec168eac26926a1e4ed12f816153a577ef5a4d18c5d8685feb6187e8b49135c28c8f7dcbd87552f8e2bcf62bc32cdfb4b3c05287bb74463445aeb78831fef4dcaefce55d0d48a3d3757ffd2371030e274c6c04cb1d13c361ede7d5752a7c2dad12a6078cfb684db5866058689b75c3fba1b67a19ac93691c8ca44ad1506e498c0bcbfbc9fabdc3bd0,english-to-german:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F972382%2F1644506%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240911%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240911T112600Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D14f8d5ceda913ae9d612e94d82162544672252d77f3eae28027d703577e845b03f7770559e2cffa8eb6455a373be30b2652ff731b83f1d831eb024564f2216c0ca12194499f2ec74e087ef711dbe5d8223271b38f396ccf9c2541a22b8f0099dc82c4f6bc91362bc4ee299cc3fd49df278a125912f67708c12990abb7059d658eec48f886c977021729b1a39b80d4bdf214eaa8cc75d1aeaa77f1ab6027f562f7d1de5dc8740e7df96c01ccdf98d27426fd3b777bca0bac73a7f13e29d8c9ea37f09851d57337369455d679b4cacb0505d0ad9f88e504dc33cd30d292526eff12a3152f8fe44d5d041d1bb58a6266630d4ecc19ec51caf863bb108f3d2483e8d,en-fr-translation-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1148896%2F1926230%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240911%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240911T112600Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D31b806613eda6a7baa00b80cf2d975a93a34c541f4eef0c86bb0ecff8b1b02806939cec30d1eb1dac5304f87c08338819805fff66c7ef91e2ec586bb68d56af5ffe12d6f6ac1511ff3ba8096b525ff099ba05f9914eca8b4fee2c2c90bf77332ea03d9ac8981c318349d1896fc1a68efa1c849f8ac8438f7cce73bb1f13639bdc96401b598b65dd99ff5afef85d02e0229b713ae8862a625b3c06938af4b608211562787429e0a74e9a99c01def3000128a3fa42edc25371e261bdf8f5a62a685e25cf8d1f1bf03f451b45325cc87553f34ce576f6a3b41acd11b93ef576b2b9e7e188d9bb64da774ff80c62171e961cb9c6ea901c3caad779dc50e1c51e63a5,english-french-translation:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2326821%2F4165279%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240911%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240911T112600Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7f70fea263cb07064c6bf92fba44098534741e71bbb97fac7c90fc6a0c2990c4eb0b3ce04a44183050881f6c99284127f68c288d4a11d5451d18823d7f57563210ddbddb6065c2bc8a53127bbcb2219c5ae83f7a4262af8ff59cab5a6c35cf184c2a55217f8492c1115e2074cf9d8168d14ecb6d57643f87fbd95f5d85f675bacbe855fc594928fe983c61d7bde85796e1a10ccf55b2517bd97c8ec2a6decefd962800f9dda355a35b86a31199f6b8f808f930692b5a9cb281ed09120dc1d6a04254e3e5e528ff88e0d9fbaa96c0c16cd5ee667beb743dcb9fdc7a4744bae5985ed236f05a627b6c133c1a6c89aee721a608215046b2a71c8df811221eb67179'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "I-HN1Fan1VsM"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#import tensorflow as tf\n",
        "\n",
        "# Détecter et initialiser le TPU\n",
        "#tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "\n",
        "# Vérifier si le TPU a été détecté\n",
        "#if tpu.cluster_spec():\n",
        "#    print(\"Connexion au TPU réussie.\")\n",
        "#    print(\"Nom du TPU :\", tpu.get_master())\n",
        "#    print(\"Adresse du TPU :\", tpu.get_master())\n",
        "#else:\n",
        "#    print(\"Impossible de se connecter au TPU.\")\n",
        "\n",
        "# Instancier la stratégie de distribution TPU\n",
        "#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "xlGXwYl41VsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(tpu_strategy)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-01T14:58:34.095307Z",
          "iopub.execute_input": "2023-08-01T14:58:34.095768Z",
          "iopub.status.idle": "2023-08-01T14:58:34.100654Z",
          "shell.execute_reply.started": "2023-08-01T14:58:34.095731Z",
          "shell.execute_reply": "2023-08-01T14:58:34.099776Z"
        },
        "_kg_hide-input": true,
        "trusted": true,
        "id": "FBH81YW_1VsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Let's try with a custom model**"
      ],
      "metadata": {
        "id": "YJLKnVCD1VsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Chargement des données\n",
        "data = pd.read_csv(\"/kaggle/input/language-translation-englishfrench/eng_-french.csv\")\n",
        "english_sentences = data[\"English words/sentences\"].tolist()\n",
        "french_sentences = data[\"French words/sentences\"].tolist()\n",
        "print(english_sentences[:5])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-04T08:48:09.972316Z",
          "iopub.execute_input": "2024-09-04T08:48:09.972971Z",
          "iopub.status.idle": "2024-09-04T08:48:20.713207Z",
          "shell.execute_reply.started": "2024-09-04T08:48:09.972944Z",
          "shell.execute_reply": "2024-09-04T08:48:20.712249Z"
        },
        "trusted": true,
        "id": "z4_cjnlo1VsW",
        "outputId": "e1312a0b-31c7-41f8-bea2-6d6dbc0bed32"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "['Hi.', 'Run!', 'Run!', 'Who?', 'Wow!']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adaptation des tokenizers aux données\n",
        "tokenizer_eng = Tokenizer()\n",
        "tokenizer_eng.fit_on_texts(english_sentences)\n",
        "eng_seq = tokenizer_eng.texts_to_sequences(english_sentences)\n",
        "\n",
        "tokenizer_fr = Tokenizer()\n",
        "tokenizer_fr.fit_on_texts(french_sentences)\n",
        "fr_seq = tokenizer_fr.texts_to_sequences(french_sentences)\n",
        "\n",
        "# Utilisation du nombre de mots dans le tokenizer pour définir l'embedding\n",
        "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
        "vocab_size_fr = len(tokenizer_fr.word_index) + 1\n",
        "\n",
        "# Padding\n",
        "max_length = max(len(seq) for seq in eng_seq + fr_seq)\n",
        "eng_seq_padded = pad_sequences(eng_seq, maxlen=max_length, padding='post')\n",
        "fr_seq_padded = pad_sequences(fr_seq, maxlen=max_length, padding='post')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-03T12:36:39.908934Z",
          "iopub.execute_input": "2024-09-03T12:36:39.909348Z",
          "iopub.status.idle": "2024-09-03T12:36:51.890873Z",
          "shell.execute_reply.started": "2024-09-03T12:36:39.909316Z",
          "shell.execute_reply": "2024-09-03T12:36:51.890025Z"
        },
        "trusted": true,
        "id": "wSSrHLy01Vsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256\n",
        "units = 512\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_length,))\n",
        "enc_emb = Embedding(input_dim=vocab_size_eng, output_dim=embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_length,))\n",
        "dec_emb_layer = Embedding(input_dim=vocab_size_fr, output_dim=embedding_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size_fr, activation='softmax')\n",
        "output = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Modèle\n",
        "model = Model([encoder_inputs, decoder_inputs], output)\n",
        "\n",
        "# Compilation du modèle\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-03T12:36:55.388189Z",
          "iopub.execute_input": "2024-09-03T12:36:55.388567Z",
          "iopub.status.idle": "2024-09-03T12:37:00.107823Z",
          "shell.execute_reply.started": "2024-09-03T12:36:55.388537Z",
          "shell.execute_reply": "2024-09-03T12:37:00.106561Z"
        },
        "trusted": true,
        "id": "FHK5UX291Vsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(eng_seq_padded, fr_seq_padded, test_size=0.2)\n",
        "model.fit([X_train, X_train], y_train, validation_data=([X_val, X_val], y_val), epochs=5, batch_size=32)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-03T12:37:05.181849Z",
          "iopub.execute_input": "2024-09-03T12:37:05.182265Z",
          "iopub.status.idle": "2024-09-03T13:22:03.093925Z",
          "shell.execute_reply.started": "2024-09-03T12:37:05.182236Z",
          "shell.execute_reply": "2024-09-03T13:22:03.092796Z"
        },
        "trusted": true,
        "id": "U6aQP3Hz1Vsc",
        "outputId": "7d67f4fa-0672-40ff-8a3f-bf354af10819"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/5\n4391/4391 [==============================] - 550s 124ms/step - loss: 0.7006 - accuracy: 0.9005 - val_loss: 0.5546 - val_accuracy: 0.9105\nEpoch 2/5\n4391/4391 [==============================] - 537s 122ms/step - loss: 0.4871 - accuracy: 0.9143 - val_loss: 0.4691 - val_accuracy: 0.9171\nEpoch 3/5\n4391/4391 [==============================] - 537s 122ms/step - loss: 0.3942 - accuracy: 0.9214 - val_loss: 0.4357 - val_accuracy: 0.9211\nEpoch 4/5\n4391/4391 [==============================] - 536s 122ms/step - loss: 0.3363 - accuracy: 0.9270 - val_loss: 0.4230 - val_accuracy: 0.9233\nEpoch 5/5\n4391/4391 [==============================] - 537s 122ms/step - loss: 0.2973 - accuracy: 0.9319 - val_loss: 0.4184 - val_accuracy: 0.9249\n",
          "output_type": "stream"
        },
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7dd906aba170>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"seq2seq_translation_v3.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-03T13:22:03.09577Z",
          "iopub.execute_input": "2024-09-03T13:22:03.096091Z",
          "iopub.status.idle": "2024-09-03T13:22:04.074113Z",
          "shell.execute_reply.started": "2024-09-03T13:22:03.096063Z",
          "shell.execute_reply": "2024-09-03T13:22:04.072853Z"
        },
        "trusted": true,
        "id": "2egXTIer1Vse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = tf.keras.models.load_model(\"seq2seq_translation_v3.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-03T11:41:53.414557Z",
          "iopub.execute_input": "2024-09-03T11:41:53.414955Z",
          "iopub.status.idle": "2024-09-03T11:41:53.419253Z",
          "shell.execute_reply.started": "2024-09-03T11:41:53.414925Z",
          "shell.execute_reply": "2024-09-03T11:41:53.418208Z"
        },
        "trusted": true,
        "id": "lfDjjY9B1Vse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence):\n",
        "    seq = tokenizer_eng.texts_to_sequences([sentence])\n",
        "    padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
        "    translated = np.argmax(model.predict([padded, padded]), axis=-1)\n",
        "\n",
        "    translated_sentence = []\n",
        "    for i in translated[0]:\n",
        "        if i in tokenizer_fr.index_word:\n",
        "            translated_sentence.append(tokenizer_fr.index_word[i])\n",
        "        else:\n",
        "            translated_sentence.append(' ')  # Token inconnu si l'indice n'est pas trouvé dans le tokenizer\n",
        "\n",
        "    return ' '.join(translated_sentence)\n",
        "\n",
        "input_sentence = \"I am tired .\"\n",
        "translated_sentence = translate_sentence(input_sentence)\n",
        "print(f\"Input: {input_sentence}\")\n",
        "print(f\"Translated: {translated_sentence}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-03T13:22:04.075911Z",
          "iopub.execute_input": "2024-09-03T13:22:04.076709Z",
          "iopub.status.idle": "2024-09-03T13:22:05.328302Z",
          "shell.execute_reply.started": "2024-09-03T13:22:04.07667Z",
          "shell.execute_reply": "2024-09-03T13:22:05.327208Z"
        },
        "trusted": true,
        "id": "l7hwrKa61Vsf",
        "outputId": "bcb0d9c6-2020-497c-a3a5-8e40db0719a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "1/1 [==============================] - 1s 1s/step\nInput: I am tired .\nTranslated: je suis fatigué                                                                                                        \n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformers**"
      ],
      "metadata": {
        "id": "5X6E06_M1Vsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-09-03T12:27:58.866611Z",
          "iopub.execute_input": "2024-09-03T12:27:58.867148Z",
          "iopub.status.idle": "2024-09-03T12:28:10.823095Z",
          "shell.execute_reply.started": "2024-09-03T12:27:58.867111Z",
          "shell.execute_reply": "2024-09-03T12:28:10.821732Z"
        },
        "trusted": true,
        "id": "_2j4vt4-1Vsg",
        "outputId": "9329b68c-06b3-49a9-b039-1fe43a4521a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "\n",
        "# Chargez le modèle T5 pré-entraîné pour la traduction anglais-français\n",
        "model_name = \"t5-base\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Chargez le tokenizer associé\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def translate_english_to_french(english_sentence):\n",
        "    # Préparez la phrase pour l'entrée du modèle (ajoutez le préfixe \"translate English to French: \")\n",
        "    input_text = \"translate English to French: \" + english_sentence\n",
        "\n",
        "    # Tokenize la phrase et encode les tokens en IDs\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Générez la traduction en français\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(input_ids=input_ids)\n",
        "\n",
        "    # Décodage de la traduction en français à partir des IDs de tokens générés\n",
        "    french_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return french_translation\n",
        "\n",
        "english_sentence = \"Hello, how was your day ?\"\n",
        "french_translation = translate_english_to_french(english_sentence)\n",
        "\n",
        "print(\"English:\", english_sentence)\n",
        "print(\"French:\", french_translation)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-03T12:13:36.513419Z",
          "iopub.execute_input": "2024-09-03T12:13:36.513833Z",
          "iopub.status.idle": "2024-09-03T12:13:48.795643Z",
          "shell.execute_reply.started": "2024-09-03T12:13:36.513795Z",
          "shell.execute_reply": "2024-09-03T12:13:48.794639Z"
        },
        "trusted": true,
        "id": "OcjUbf5Q1Vsh",
        "outputId": "430b2aaf-63a8-4621-e9eb-47d8ee0b8cec",
        "colab": {
          "referenced_widgets": [
            "e67e048737d44254a7b8a2ceeda481ea",
            "771bcde759f94f8697b2fd5fc6444c40",
            "39d07864e28d4ea28ed638f458f0b4f2",
            "695b733ff56847bba5079647a790877d"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e67e048737d44254a7b8a2ceeda481ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "771bcde759f94f8697b2fd5fc6444c40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39d07864e28d4ea28ed638f458f0b4f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "695b733ff56847bba5079647a790877d"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "English: Hello, how was your day ?\nFrench: Bonjour, quelle a été votre journée?\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentence = \"hello ?\"\n",
        "french_translation = translate_english_to_french(english_sentence)\n",
        "\n",
        "print(\"English:\", english_sentence)\n",
        "print(\"French:\", french_translation)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-03T12:28:24.14656Z",
          "iopub.execute_input": "2024-09-03T12:28:24.147324Z",
          "iopub.status.idle": "2024-09-03T12:28:24.416624Z",
          "shell.execute_reply.started": "2024-09-03T12:28:24.147288Z",
          "shell.execute_reply": "2024-09-03T12:28:24.415481Z"
        },
        "trusted": true,
        "id": "pvgmf99s1Vsh",
        "outputId": "89e6d11f-3f16-4915-dcb6-3c5585ad6182"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "English: hello ?\nFrench: Bonjour?\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got better result with transformers models. Custom models help us to understand how it work."
      ],
      "metadata": {
        "id": "GObAqjpe1Vsi"
      }
    }
  ]
}